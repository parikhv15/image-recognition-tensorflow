{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for purposes of reproducibility\n",
    "seed = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "class_num = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create a model. Sequential is most common\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the convolutional layer with\n",
    "# 1. 32 Filters/Channels\n",
    "# 2. Size of filter is (3X3)\n",
    "# 3. Input shape\n",
    "# 4. Padding, 'same' in this scenario\n",
    "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
    "\n",
    "# Adding activation function `relu`, most commonly used\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# or you can do the below:\n",
    "# model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))\n",
    "\n",
    "# Now we will make a dropout layer to prevent overfitting, which functions by randomly eliminating\n",
    "# some of the connections between the layers (0.2 means it drops 20% of the existing connections)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Batch Normalization normalizes the inputs heading into the next layer,\n",
    "# ensuring that the network always creates activations with the same distribution\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the pooling layer, as discussed before this helps make the image classifier\n",
    "# more robust so it can learn relevant patterns.\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now repeat these layers to give your network more representations to work off\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Flatten the data\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we make use of the Dense import and create the first densely connected layer.\n",
    "# We need to specify the number of neurons in the dense layer. Note that the numbers\n",
    "# of neurons in succeeding layers decreases, eventually approaching the same number of\n",
    "# neurons as there are classes in the dataset (in this case 10). The kernel constraint\n",
    "# can regularize the data as it learns, another thing that helps prevent overfitting.\n",
    "# This is why we imported maxnorm earlier.\n",
    "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this final layer, we pass in the number of classes for the number of neurons.\n",
    "# Each neuron represents a class, and the output of this layer will be a 10 neuron\n",
    "# vector with each neuron storing some probability that the image in question belongs\n",
    "# to the class it represents.\n",
    "model.add(Dense(class_num))\n",
    "\n",
    "# Finally, the softmax activation function selects the neuron with the highest\n",
    "# probability as its output, voting that the image belongs to that class.\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "# The optimizer is what will tune the weights in your network to approach the\n",
    "# point of lowest loss. The Adam algorithm is one of the most commonly used\n",
    "# optimizers because it gives great performance\n",
    "epochs = 25\n",
    "optimizer = 'adam'\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all we have to do is call the fit() function on the model and pass in the chosen parameters.\n",
    "numpy.random.seed(seed)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
